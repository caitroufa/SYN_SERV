{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5dd4d6-195b-42f7-a4cd-e92e24ad14d8",
   "metadata": {},
   "source": [
    "# Fraction Skill Score - Dataframe Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04c31a47-6ef5-40d4-9154-abba0868093f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pysteps configuration file found at: /anaconda3/envs/pyEAE/lib/python3.9/site-packages/pysteps/pystepsrc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import xoak\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import regionmask\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import glob\n",
    "%matplotlib inline\n",
    "import pysteps\n",
    "import ipywidgets as widgets\n",
    "from copy import deepcopy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "788c234f-78c5-4bd7-992a-dcbe70d76ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Conv(ds):\n",
    "    \n",
    "    '''\n",
    "    A defintion  \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds: xarray dataset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ds: returns a coarsened data with lat/long\n",
    "    \n",
    "    '''\n",
    "    #load the geog file produce for WRF-BCC\n",
    "    geog = xr.open_dataset(\"/home/scratch/WRF_BCC/geography/geo_em.d01.nc\")\n",
    "    geog = geog[['CLAT', 'CLONG']].coarsen(south_north=20, west_east=20, boundary='trim').mean()\n",
    "    \n",
    "    #coarsen the dataset to ~80-km\n",
    "    ds = ds.coarsen(south_north=20, west_east=20, boundary='trim').sum()\n",
    "    ds = xr.merge([ds, geog.squeeze()])\n",
    "    ds = ds.rename({\"CLONG\": 'lon', 'CLAT': 'lat'})\n",
    "\n",
    "    #assign lat/lon values of coarsen data and set index\n",
    "    ds = ds.assign_coords({'x': ds.west_east, 'y': ds.south_north})\n",
    "    ds = ds.assign_coords({'lon': ds.lon, 'lat': ds.lat})\n",
    "    ds.xoak.set_index(['lat', 'lon'], 'sklearn_geo_balltree')\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3ef884-a3a7-4680-9ae2-854160f668bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open dummy xarray dataset\n",
    "ds = xr.open_dataset('/home/scratch/WRF_BCC/severe_weather/UP_HELI_MAX/historical/1990-1991/UP_HELI_MAX_historical-1990-1991_1990-10-01.nc').sel(Time='1990-10-01T00:00:00.000000000')\n",
    "ds_copy = ds.copy()\n",
    "ds_copy_cor = Data_Conv(ds_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "138d7880-48db-4ed2-baaf-12e4ce077274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load an USA shapefile\n",
    "usa = gpd.read_file(\"/home/jcorner1/Unidata/shapefiles/smoothing_econus.shp\")\n",
    "\n",
    "#mask the data out\n",
    "state_mask = regionmask.mask_geopandas(usa, ds_copy_cor.lon, ds_copy_cor.lat)\n",
    "ma = state_mask.values\n",
    "ma[~np.isnan(ma)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e81873d-bf21-4067-9d28-11628de22a03",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6b5e79-af4c-40d9-a460-3fee68a290b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the proxy data\n",
    "#create lite of the proxies\n",
    "varis = ['UVV', 'UH', 'DVV']\n",
    "\n",
    "for var in varis:\n",
    "    \n",
    "    #load wrf-bcc dataset\n",
    "    locals()[f'df_{var}'] = pd.read_csv(f'/home/scratch/jcorner1/syn_sev/dataframes/HIST_{var}_REFC_regrid_threshold_dataframe.csv')\n",
    "    locals()[f'df_{var}']['Time'] = pd.to_datetime(locals()[f'df_{var}']['Time'])\n",
    "    \n",
    "df_DVV['DVV'] = np.absolute(df_DVV['DVV'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec221e60-aea3-4753-a0dc-8d26336f19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the hazard data\n",
    "#create list of the hazards\n",
    "hazs = ['tor', 'hail', 'wind']\n",
    "\n",
    "#iterate through the hazards\n",
    "for haz in hazs:\n",
    "    df_obs = pd.read_csv(f'/home/scratch/jcorner1/syn_sev/dataframes/SPC_LSRs_regridded_{haz}_fix.csv')\n",
    "    \n",
    "    #add the lat and lon columns based on values in the nearest column\n",
    "    lat = []\n",
    "    lon = []\n",
    "\n",
    "    #iterate through each row to find the lat/lon\n",
    "    for rid, row in df_obs.iterrows():\n",
    "        lat.append(row.Nearest.split()[1])\n",
    "        lon.append(row.Nearest.split()[7])\n",
    "\n",
    "    #append the values to the dataframe\n",
    "    df_obs['Lat'] = lat\n",
    "    df_obs['Lon'] = lon\n",
    "\n",
    "    #combine date and time to a datetime object\n",
    "    df_obs['datetime'] = pd.to_datetime(df_obs['date'] + ' ' + df_obs['time']) + pd.DateOffset(hours=6)\n",
    "    df_obs['con_date'] = (df_obs['datetime'] + pd.DateOffset(hours=12)).dt.date\n",
    "\n",
    "    #drop duplicates to convert to convective days\n",
    "    df_obs = df_obs.drop_duplicates(subset=['Lat','Lon','con_date'])\n",
    "    locals()[f'df_{haz}'] = df_obs\n",
    "    \n",
    "df_all = pd.concat([df_tor, df_wind, df_hail])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cec6948-e1ca-477a-b85a-9417a06dff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the all hazard to the list.\n",
    "hazs = hazs + ['all']\n",
    "\n",
    "#iterate through the dataframes.\n",
    "for haz in hazs:\n",
    "\n",
    "    #create an array of zeros\n",
    "    ds_copy_cor = Data_Conv(ds_copy)\n",
    "    zeros = np.zeros((ds_copy_cor.UP_HELI_MAX.values.shape))\n",
    "\n",
    "    #iterate through each row of the dataframe to and add one for each instance\n",
    "    for rid, row in locals()[f'df_{haz}'].iterrows():\n",
    "        y1, x1 = np.where(ds_copy_cor.lat.values == float(row.Lat))\n",
    "        y2, x2 = np.where(ds_copy_cor.lon.values == float(row.Lon))\n",
    "        zeros[y1[0], x2[0]] = zeros[y1[0], x2[0]] + 1\n",
    "\n",
    "    locals()[f'ds_{haz}'] = ds_copy_cor.assign(UP_HELI_MAX = (('south_north', 'west_east'), zeros * ma))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6a81f-161e-4043-b43d-30c34c580218",
   "metadata": {},
   "source": [
    "### Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3ea9fcd-69f6-4e3d-a960-51a22fbdc44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UVV:18, DBZ:40\n",
      "UVV:19, DBZ:40\n",
      "UVV:20, DBZ:40\n",
      "UVV:21, DBZ:40\n",
      "UVV:22, DBZ:40\n",
      "UVV:23, DBZ:40\n",
      "UVV:24, DBZ:40\n",
      "UVV:25, DBZ:40\n",
      "UVV:26, DBZ:40\n",
      "UVV:27, DBZ:40\n",
      "UVV:28, DBZ:40\n",
      "UVV:29, DBZ:40\n",
      "UVV:30, DBZ:40\n",
      "UVV:18, DBZ:45\n",
      "UVV:19, DBZ:45\n",
      "UVV:20, DBZ:45\n",
      "UVV:21, DBZ:45\n",
      "UVV:22, DBZ:45\n",
      "UVV:23, DBZ:45\n",
      "UVV:24, DBZ:45\n",
      "UVV:25, DBZ:45\n",
      "UVV:26, DBZ:45\n",
      "UVV:27, DBZ:45\n",
      "UVV:28, DBZ:45\n",
      "UVV:29, DBZ:45\n",
      "UVV:30, DBZ:45\n",
      "UVV:18, DBZ:50\n",
      "UVV:19, DBZ:50\n",
      "UVV:20, DBZ:50\n",
      "UVV:21, DBZ:50\n",
      "UVV:22, DBZ:50\n",
      "UVV:23, DBZ:50\n",
      "UVV:24, DBZ:50\n",
      "UVV:25, DBZ:50\n",
      "UVV:26, DBZ:50\n",
      "UVV:27, DBZ:50\n",
      "UVV:28, DBZ:50\n",
      "UVV:30, DBZ:50\n",
      "UVV:18, DBZ:55\n",
      "UVV:19, DBZ:55\n",
      "UVV:20, DBZ:55\n",
      "UVV:21, DBZ:55\n",
      "UVV:22, DBZ:55\n",
      "UVV:23, DBZ:55\n",
      "UVV:24, DBZ:55\n",
      "UVV:25, DBZ:55\n",
      "UVV:26, DBZ:55\n",
      "UVV:27, DBZ:55\n",
      "UVV:28, DBZ:55\n",
      "UVV:29, DBZ:55\n",
      "UVV:30, DBZ:55\n",
      "UH:35, DBZ:40\n",
      "UH:40, DBZ:40\n",
      "UH:45, DBZ:40\n",
      "UH:50, DBZ:40\n",
      "UH:55, DBZ:40\n",
      "UH:60, DBZ:40\n",
      "UH:65, DBZ:40\n",
      "UH:70, DBZ:40\n",
      "UH:75, DBZ:40\n",
      "UH:80, DBZ:40\n",
      "UH:85, DBZ:40\n",
      "UH:35, DBZ:45\n",
      "UH:40, DBZ:45\n",
      "UH:45, DBZ:45\n",
      "UH:50, DBZ:45\n",
      "UH:55, DBZ:45\n",
      "UH:60, DBZ:45\n",
      "UH:65, DBZ:45\n",
      "UH:70, DBZ:45\n",
      "UH:75, DBZ:45\n",
      "UH:80, DBZ:45\n",
      "UH:85, DBZ:45\n",
      "UH:35, DBZ:50\n",
      "UH:40, DBZ:50\n",
      "UH:45, DBZ:50\n",
      "UH:50, DBZ:50\n",
      "UH:55, DBZ:50\n",
      "UH:60, DBZ:50\n",
      "UH:65, DBZ:50\n",
      "UH:70, DBZ:50\n",
      "UH:75, DBZ:50\n",
      "UH:80, DBZ:50\n",
      "UH:85, DBZ:50\n",
      "UH:35, DBZ:55\n",
      "UH:40, DBZ:55\n",
      "UH:45, DBZ:55\n",
      "UH:50, DBZ:55\n",
      "UH:55, DBZ:55\n",
      "UH:60, DBZ:55\n",
      "UH:65, DBZ:55\n",
      "UH:70, DBZ:55\n",
      "UH:75, DBZ:55\n",
      "UH:80, DBZ:55\n",
      "UH:85, DBZ:55\n",
      "DVV:5, DBZ:40\n",
      "DVV:6, DBZ:40\n",
      "DVV:7, DBZ:40\n",
      "DVV:8, DBZ:40\n",
      "DVV:9, DBZ:40\n",
      "DVV:10, DBZ:40\n",
      "DVV:5, DBZ:45\n",
      "DVV:6, DBZ:45\n",
      "DVV:7, DBZ:45\n",
      "DVV:8, DBZ:45\n",
      "DVV:9, DBZ:45\n",
      "DVV:10, DBZ:45\n",
      "DVV:5, DBZ:50\n",
      "DVV:6, DBZ:50\n",
      "DVV:7, DBZ:50\n",
      "DVV:8, DBZ:50\n",
      "DVV:9, DBZ:50\n",
      "DVV:10, DBZ:50\n",
      "DVV:5, DBZ:55\n",
      "DVV:6, DBZ:55\n",
      "DVV:7, DBZ:55\n",
      "DVV:8, DBZ:55\n",
      "DVV:9, DBZ:55\n",
      "DVV:10, DBZ:55\n"
     ]
    }
   ],
   "source": [
    "thrs_vals = []\n",
    "\n",
    "for i in range(len(varis)):\n",
    "    df = pd.DataFrame(columns=['DBZ', var, 'FSS_tor', 'FSS_wind', 'FSS_hail', 'FSS_all'])\n",
    "    \n",
    "    for refc in np.arange(40, 56, 5):\n",
    "        \n",
    "        #variable logic statements\n",
    "        if varis[i] == 'UH':\n",
    "            step = 5\n",
    "            lb = 35\n",
    "            ub = 85\n",
    "            \n",
    "        elif varis[i] == 'UVV':\n",
    "            step = 1\n",
    "            lb = 18\n",
    "            ub = 30\n",
    "            \n",
    "        elif varis[i] == 'DVV':\n",
    "            step = 1\n",
    "            lb = 5\n",
    "            ub = 10\n",
    "\n",
    "        for var_val in np.arange(lb, ub+1, step):\n",
    "            print(f'{varis[i]}:{var_val}, DBZ:{refc}')\n",
    "            \n",
    "            #subset the dataframe based on current threshold values\n",
    "            df_wrf = locals()[f'df_{varis[i]}']\n",
    "            df_sub = df_wrf[((df_wrf['DBZ'] >= refc) & (df_wrf[varis[i]] >= var_val))]\n",
    "            \n",
    "            #\n",
    "            geog = xr.open_dataset(\"/home/scratch/WRF_BCC/geography/geo_em.d01.nc\")\n",
    "            uhs = []\n",
    "\n",
    "            #\n",
    "            for did, day in df_sub.resample('24h', origin='1990-10-01 12:00:00', on='Time'):\n",
    "\n",
    "                #\n",
    "                results = np.zeros(shape=(899, 1399))\n",
    "                results[day.y.values, day.x.values] = 1\n",
    "                uh_uvv = deepcopy(geog)\n",
    "                uh_uvv = uh_uvv.assign_coords({'Time': np.array([did])})\n",
    "                uh_uvv = uh_uvv.assign(UH_VVV_DAYS = (('Time', 'south_north', 'west_east'), np.expand_dims(results, axis=0)))\n",
    "                uh_uvv = uh_uvv[['CLAT', 'CLONG', 'UH_VVV_DAYS']].coarsen(south_north=20, west_east=20, boundary='trim').mean()\n",
    "                uh_uvv['UH_VVV_DAYS'] = 1*(uh_uvv['UH_VVV_DAYS'] > 0)\n",
    "                uhs.append(uh_uvv)\n",
    "\n",
    "            #\n",
    "            uhs = xr.concat(uhs, dim='Time')\n",
    "\n",
    "            #verify with FSS!\n",
    "            fss_tor = pysteps.verification.spatialscores.fss(np.sum(uhs.UH_VVV_DAYS.values, axis=0), ds_tor.UP_HELI_MAX.values, 20, 200)\n",
    "            fss_wind = pysteps.verification.spatialscores.fss(np.sum(uhs.UH_VVV_DAYS.values, axis=0), ds_wind.UP_HELI_MAX.values, 20, 200)\n",
    "            fss_hail = pysteps.verification.spatialscores.fss(np.sum(uhs.UH_VVV_DAYS.values, axis=0), ds_hail.UP_HELI_MAX.values, 20, 200)\n",
    "            fss_all = pysteps.verification.spatialscores.fss(np.sum(uhs.UH_VVV_DAYS.values, axis=0), ds_all.UP_HELI_MAX.values, 20, 200)\n",
    "            \n",
    "            df = df.append({'DBZ': refc, var: var_val, 'FSS_tor': fss_tor\n",
    "                            , 'FSS_wind':fss_wind, 'FSS_hail':fss_hail, 'FSS_all':fss_all}, ignore_index=True)\n",
    "\n",
    "\n",
    "    #create a new dataframe\n",
    "    df.to_csv(f'/home/scratch/jcorner1/syn_sev/dataframes/FSS/FSS_{varis[i]}_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a1cdf4-bfed-480a-9fd3-750c7ec969ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyEAE]",
   "language": "python",
   "name": "conda-env-pyEAE-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
